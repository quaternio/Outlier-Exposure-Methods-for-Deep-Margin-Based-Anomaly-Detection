{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\")\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import copy\n",
    "from torchvision.models import resnet18, resnet34, resnet50, efficientnet_b1\n",
    "from torch.utils import data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, RMSprop\n",
    "import numpy as np\n",
    "from large_margin import LargeMarginLoss\n",
    "import time\n",
    "import argparse\n",
    "from data import build_split_datasets\n",
    "import pickle as pkl\n",
    "from random_split_generator import FourWayClassSplit\n",
    "from torch.utils.data import ConcatDataset\n",
    "import datetime\n",
    "from experiment import FeatureExtractor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "from large_margin import _get_grad\n",
    "\n",
    "def test_distance(logits, features, device):\n",
    "    eps = 1e-8\n",
    "    prob = F.softmax(logits, dim=1)\n",
    "\n",
    "    max_indices = torch.argmax(prob, dim=1)\n",
    "    pseudo_correct_prob, _ = torch.max(prob, dim=1, keepdim=True)\n",
    "\n",
    "    pseudo_other_prob = torch.zeros(prob.shape).to(device)\n",
    "    pseudo_other_prob.copy_(prob)\n",
    "    pseudo_other_prob[torch.arange(prob.shape[0]),max_indices] = 0.\n",
    "\n",
    "    # Grabs the next most likely class probabilities\n",
    "    if top_k > 1:\n",
    "        topk_prob, _ = pseudo_other_prob.topk(top_k, dim=1)\n",
    "    else:\n",
    "        topk_prob, _ = pseudo_other_prob.max(dim=1, keepdim=True)\n",
    "\n",
    "    pseudo_diff_prob = pseudo_correct_prob - topk_prob\n",
    "\n",
    "    for i, feature_map in enumerate(features):\n",
    "        if i == len(features)-1:\n",
    "            diff_grad = torch.stack([_get_grad(pseudo_diff_prob[:, i], feature_map) for i in range(top_k)],\n",
    "                                dim=1)\n",
    "            diff_gradnorm = torch.norm(diff_grad, p=2, dim=2)\n",
    "            diff_gradnorm.detach_()\n",
    "            distance = pseudo_diff_prob / (diff_gradnorm + eps)\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "def split_test_lm_ls(model, top_k, id_loader, ood_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    anomaly_index = 10\n",
    "    num_classes = 10\n",
    "    anom_pred = []\n",
    "    anom_labels = []\n",
    "    anom_score_sequence = []\n",
    "    pred_sequence = []\n",
    "    target_sequence = []\n",
    "    \n",
    "    for batch_idx, (id_data, id_target) in enumerate(id_loader):\n",
    "        id_one_hot = torch.zeros(len(id_target), num_classes).scatter_(1, id_target.unsqueeze(1), 1.).float()\n",
    "        id_one_hot = id_one_hot.to(device)\n",
    "        id_data, id_target   = id_data.to(device), id_target.to(device)\n",
    "        model.clear_features()\n",
    "        id_data = id_data.to(device)\n",
    "        id_output, id_features  = model(id_data)\n",
    "        for id_feature in id_features:\n",
    "            id_feature.retain_grad()\n",
    "\n",
    "        ###########################\n",
    "        # ID Distance Computation #\n",
    "        ###########################\n",
    "        raw_id_distance = test_distance(id_output, id_features, device)\n",
    "        id_distance = torch.abs(raw_id_distance)\n",
    "\n",
    "        # Compute number of correctly classified id instances\n",
    "        id_pred   = id_output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "        _, id_idx = id_output.max(dim=1)\n",
    "        correct += (id_idx == id_target).sum().item()\n",
    "        id_anom_pred = [1. if id_pred[i] == anomaly_index else 0. for i in range(len(id_pred))]\n",
    "\n",
    "        # Concatenate the list (order matters here)\n",
    "        id_batch_anom_pred = id_anom_pred\n",
    "        anom_pred = anom_pred + id_batch_anom_pred\n",
    "        pred_sequence.append(id_pred)\n",
    "        target_sequence.append(id_target)\n",
    "\n",
    "        # Compute anomaly scores\n",
    "        # Use discriminant function to compute id_scores\n",
    "        # id_distance = torch.abs(id_distance)\n",
    "        id_scores, _ = torch.max(-1 * id_distance, dim=1)\n",
    "\n",
    "        # Detaching is important here because it removes these scores from the computational graph\n",
    "        anom_score_sequence.append(id_scores.detach().cpu())\n",
    "        for i in range(len(id_target)):\n",
    "            # 0 indicates \"nominal\"\n",
    "            anom_labels.append(0.)\n",
    "\n",
    "    for batch_idx, (ood_data, ood_target) in enumerate(ood_loader):\n",
    "        ood_target = anomaly_index * torch.ones_like(ood_target)\n",
    "        ood_one_hot = (1/id_one_hot.shape[1])*torch.ones((len(ood_target), id_one_hot.shape[1])).to(device)\n",
    "        ood_one_hot = ood_one_hot.to(device)\n",
    "        ood_data, ood_target = ood_data.to(device), ood_target.to(device)\n",
    "        model.clear_features()\n",
    "        ood_output, ood_features = model(ood_data)\n",
    "        for ood_feature in ood_features:\n",
    "            ood_feature.retain_grad()\n",
    "\n",
    "        ############################\n",
    "        # OOD Distance Computation #\n",
    "        ############################\n",
    "        raw_ood_distance = test_distance(ood_output, ood_features, device)\n",
    "        ood_distance = torch.abs(raw_ood_distance)\n",
    "\n",
    "        ood_pred   = ood_output.argmax(dim=1, keepdim=True)\n",
    "        ood_anom_pred = [1. if ood_pred[i] == anomaly_index else 0. for i in range(len(ood_pred))]\n",
    "        \n",
    "        # Concatenate the list (order matters here)\n",
    "        ood_batch_anom_pred = ood_anom_pred\n",
    "        anom_pred = anom_pred + ood_batch_anom_pred\n",
    "        pred_sequence.append(ood_pred)\n",
    "        target_sequence.append(ood_target)\n",
    "\n",
    "        ood_scores, _ = torch.max(-1 * ood_distance, dim=1)\n",
    "\n",
    "        # Detaching is important here because it removes these scores from computational graph\n",
    "        anom_score_sequence.append(ood_scores.detach().cpu())\n",
    "        for i in range(len(ood_target)):\n",
    "            # 1 indicates \"anomaly\"\n",
    "            anom_labels.append(1.)\n",
    "\n",
    "    anom_scores = torch.hstack(anom_score_sequence).cpu().detach().numpy()\n",
    "    anom_labels = np.asarray(anom_labels)\n",
    "    anom_pred = np.asarray(anom_pred)\n",
    "    pred = torch.vstack(pred_sequence).cpu().detach().numpy()\n",
    "    pred = np.ndarray.flatten(pred)\n",
    "    targets = torch.hstack(target_sequence).cpu().detach().numpy()\n",
    "\n",
    "    AUROC = roc_auc_score(anom_labels, anom_scores)\n",
    "\n",
    "    accuracy = 100. * correct / len(id_loader.dataset)\n",
    "    print('Test set: Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        correct, len(id_loader.dataset), accuracy))\n",
    "    print('Test Set: AUROC: {}\\n'.format(AUROC))\n",
    "\n",
    "    return accuracy, AUROC, id_distance, ood_distance\n",
    "\n",
    "\n",
    "PATH = '/nfs/stak/users/noelt/Documents/Project/noelt_masters_project/models/val_baseline_margin_new_class_test/epoch_259_split_0.pth'\n",
    "batch_size=256\n",
    "\n",
    "# Right now, hardcoded to use CIFAR-100, with hardcoded splits\n",
    "# defined and generated in random_split_generator.py\n",
    "id_data, ood_data = build_split_datasets(0, False)\n",
    "\n",
    "id_train_data, id_val_data, id_test_data    = id_data\n",
    "ood_train_data, ood_val_data, ood_test_data = ood_data\n",
    "\n",
    "# Constructing Dataloaders\n",
    "id_train_loader = data.DataLoader(id_train_data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "id_val_loader   = data.DataLoader(id_val_data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "id_test_loader   = data.DataLoader(id_test_data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "ood_train_loader   = data.DataLoader(ood_train_data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "ood_val_loader   = data.DataLoader(ood_val_data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "ood_test_loader   = data.DataLoader(ood_test_data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "model = FeatureExtractor(\"efficientnet_b1\", 10)\n",
    "model.load_state_dict(torch.load(PATH)['model_state_dict'])\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "top_k=1\n",
    "\n",
    "acc, auc, id_dist, ood_dist = split_test_lm_ls(model, top_k, id_val_loader, ood_val_loader, device)\n",
    "\n",
    "print(\"Accuracy: {}\".format(acc))\n",
    "print(\"AUROC: {}\".format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 ('project': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3add45fc69f05cc29cf9d44865c28a2c1800260c2b05417b30b97f2c4a51861"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
