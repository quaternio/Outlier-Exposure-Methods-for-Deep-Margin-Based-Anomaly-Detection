{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\")\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import copy\n",
    "from torchvision.models import resnet18, resnet34, resnet50, efficientnet_b1\n",
    "from torch.utils import data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, RMSprop\n",
    "import numpy as np\n",
    "from large_margin import LargeMarginLoss\n",
    "import time\n",
    "import argparse\n",
    "from data import build_split_datasets\n",
    "import pickle as pkl\n",
    "from random_split_generator import FourWayClassSplit\n",
    "from torch.utils.data import ConcatDataset\n",
    "import datetime\n",
    "from experiment import FeatureExtractor\n",
    "from sklearn.metrics import roc_auc_score, confusion_matrix\n",
    "import pandas as pd\n",
    "from large_margin import _get_grad\n",
    "\n",
    "def test_distance(logits, features, top_k, device):\n",
    "    eps = 1e-8\n",
    "    prob = F.softmax(logits, dim=1)\n",
    "\n",
    "    max_indices = torch.argmax(prob, dim=1)\n",
    "    pseudo_correct_prob, _ = torch.max(prob, dim=1, keepdim=True)\n",
    "\n",
    "    pseudo_other_prob = torch.zeros(prob.shape).to(device)\n",
    "    pseudo_other_prob.copy_(prob)\n",
    "    pseudo_other_prob[torch.arange(prob.shape[0]),max_indices] = 0.\n",
    "\n",
    "    # Grabs the next most likely class probabilities\n",
    "    if top_k > 1:\n",
    "        topk_prob, _ = pseudo_other_prob.topk(top_k, dim=1)\n",
    "    else:\n",
    "        topk_prob, _ = pseudo_other_prob.max(dim=1, keepdim=True)\n",
    "\n",
    "    pseudo_diff_prob = pseudo_correct_prob - topk_prob\n",
    "\n",
    "    for i, feature_map in enumerate(features):\n",
    "        if i == len(features)-1:\n",
    "            diff_grad = torch.stack([_get_grad(pseudo_diff_prob[:, i], feature_map) for i in range(top_k)],\n",
    "                                dim=1)\n",
    "            diff_gradnorm = torch.norm(diff_grad, p=2, dim=2)\n",
    "            diff_gradnorm.detach_()\n",
    "            distance = pseudo_diff_prob / (diff_gradnorm + eps)\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "def test_lm_ls(model, top_k, id_loader, ood_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    anomaly_index = 10\n",
    "    num_classes = 10\n",
    "    anom_pred = []\n",
    "    anom_labels = []\n",
    "    margin_anom_score_sequence = []\n",
    "    max_logit_anom_score_sequence = []\n",
    "    pred_sequence = []\n",
    "    target_sequence = []\n",
    "    \n",
    "    for batch_idx, (id_data, id_target) in enumerate(id_loader):\n",
    "        id_one_hot = torch.zeros(len(id_target), num_classes).scatter_(1, id_target.unsqueeze(1), 1.).float()\n",
    "        id_one_hot = id_one_hot.to(device)\n",
    "        id_data, id_target   = id_data.to(device), id_target.to(device)\n",
    "        model.clear_features()\n",
    "        id_data = id_data.to(device)\n",
    "        id_output, id_features  = model(id_data)\n",
    "        for id_feature in id_features:\n",
    "            id_feature.retain_grad()\n",
    "\n",
    "        ###########################\n",
    "        # ID Distance Computation #\n",
    "        ###########################\n",
    "        raw_id_distance = test_distance(id_output, id_features, top_k, device)\n",
    "        id_distance = torch.abs(raw_id_distance)\n",
    "\n",
    "        # Compute number of correctly classified id instances\n",
    "        id_pred   = id_output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "        _, id_idx = id_output.max(dim=1)\n",
    "        correct += (id_idx == id_target).sum().item()\n",
    "        id_anom_pred = [1. if id_pred[i] == anomaly_index else 0. for i in range(len(id_pred))]\n",
    "\n",
    "        # Concatenate the list (order matters here)\n",
    "        id_batch_anom_pred = id_anom_pred\n",
    "        anom_pred = anom_pred + id_batch_anom_pred\n",
    "        pred_sequence.append(id_pred)\n",
    "        target_sequence.append(id_target)\n",
    "\n",
    "        # Compute anomaly scores\n",
    "        # Use discriminant function to compute id_scores\n",
    "        # id_distance = torch.abs(id_distance)\n",
    "        margin_id_scores, _ = torch.max(-1 * id_distance, dim=1)\n",
    "        max_logit_id_scores, _ = torch.max(id_output, dim=1)\n",
    "        max_logit_id_scores = -1 * max_logit_id_scores\n",
    "\n",
    "        # Detaching is important here because it removes these scores from the computational graph\n",
    "        margin_anom_score_sequence.append(margin_id_scores.detach().cpu())\n",
    "        max_logit_anom_score_sequence.append(max_logit_id_scores.detach().cpu())\n",
    "        for i in range(len(id_target)):\n",
    "            # 0 indicates \"nominal\"\n",
    "            anom_labels.append(0.)\n",
    "\n",
    "    for batch_idx, (ood_data, ood_target) in enumerate(ood_loader):\n",
    "        ood_target = anomaly_index * torch.ones_like(ood_target)\n",
    "        ood_one_hot = (1/id_one_hot.shape[1])*torch.ones((len(ood_target), id_one_hot.shape[1])).to(device)\n",
    "        ood_one_hot = ood_one_hot.to(device)\n",
    "        ood_data, ood_target = ood_data.to(device), ood_target.to(device)\n",
    "        model.clear_features()\n",
    "        ood_output, ood_features = model(ood_data)\n",
    "        for ood_feature in ood_features:\n",
    "            ood_feature.retain_grad()\n",
    "\n",
    "        ############################\n",
    "        # OOD Distance Computation #\n",
    "        ############################\n",
    "        raw_ood_distance = test_distance(ood_output, ood_features, top_k, device)\n",
    "        ood_distance = torch.abs(raw_ood_distance)\n",
    "\n",
    "        ood_pred   = ood_output.argmax(dim=1, keepdim=True)\n",
    "        ood_anom_pred = [1. if ood_pred[i] == anomaly_index else 0. for i in range(len(ood_pred))]\n",
    "        \n",
    "        # Concatenate the list (order matters here)\n",
    "        ood_batch_anom_pred = ood_anom_pred\n",
    "        anom_pred = anom_pred + ood_batch_anom_pred\n",
    "        pred_sequence.append(ood_pred)\n",
    "        target_sequence.append(ood_target)\n",
    "\n",
    "        margin_ood_scores, _ = torch.max(-1 * ood_distance, dim=1)\n",
    "        max_logit_ood_scores, _ = torch.max(ood_output, dim=1)\n",
    "        max_logit_ood_scores = -1 * max_logit_ood_scores\n",
    "\n",
    "        # Detaching is important here because it removes these scores from computational graph\n",
    "        margin_anom_score_sequence.append(margin_ood_scores.detach().cpu())\n",
    "        max_logit_anom_score_sequence.append(max_logit_ood_scores.detach().cpu())\n",
    "        for i in range(len(ood_target)):\n",
    "            # 1 indicates \"anomaly\"\n",
    "            anom_labels.append(1.)\n",
    "\n",
    "    margin_anom_scores = torch.hstack(margin_anom_score_sequence).cpu().detach().numpy()\n",
    "    max_logit_anom_scores = torch.hstack(max_logit_anom_score_sequence).cpu().detach().numpy()\n",
    "    anom_labels = np.asarray(anom_labels)\n",
    "    anom_pred = np.asarray(anom_pred)\n",
    "    pred = torch.vstack(pred_sequence).cpu().detach().numpy()\n",
    "    pred = np.ndarray.flatten(pred)\n",
    "    targets = torch.hstack(target_sequence).cpu().detach().numpy()\n",
    "\n",
    "    margin_AUROC    = roc_auc_score(anom_labels, margin_anom_scores)\n",
    "    max_logit_AUROC = roc_auc_score(anom_labels, max_logit_anom_scores) \n",
    "\n",
    "    accuracy = 100. * correct / len(id_loader.dataset)\n",
    "    print('Test set: Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        correct, len(id_loader.dataset), accuracy))\n",
    "    print('Test Set: margin AUROC: {}\\n'.format(margin_AUROC))\n",
    "    print('Test Set: max logit AUROC: {}\\n'.format(max_logit_AUROC))\n",
    "\n",
    "    return accuracy, margin_AUROC, max_logit_AUROC\n",
    "\n",
    "\n",
    "# TODO: Verify Done\n",
    "# This works as a test function for our baseline\n",
    "def test_ce_ls(model, id_loader, ood_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    anom_labels = []\n",
    "    anom_score_sequence = []\n",
    "    with torch.no_grad(): \n",
    "        for batch_idx, (id_data, id_target) in enumerate(id_loader):\n",
    "            id_data, id_target   = id_data.to(device), id_target.to(device)\n",
    "            id_output, _  = model(id_data)\n",
    "            \n",
    "            # Compute number of correctly classified id instances\n",
    "            id_pred = id_output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            _, id_idx = id_output.max(dim=1)\n",
    "            correct += (id_idx == id_target).sum().item()\n",
    "\n",
    "            # Compute anomaly scores\n",
    "            pos_id_scores, _ = torch.max(id_output, dim=1)\n",
    "            id_scores = -1 * pos_id_scores\n",
    "            anom_score_sequence.append(id_scores)\n",
    "            for i in range(len(id_target)):\n",
    "                # 0 indicates \"nominal\"\n",
    "                anom_labels.append(0.)\n",
    "\n",
    "        for batch_idx, (ood_data, ood_target) in enumerate(ood_loader):\n",
    "            ood_data, ood_target = ood_data.to(device), ood_target.to(device)\n",
    "            ood_output, _ = model(ood_data)\n",
    "            \n",
    "            # Compute anomaly scores\n",
    "            pos_ood_scores, _ = torch.max(ood_output, dim=1)\n",
    "            ood_scores = -1 * pos_ood_scores\n",
    "            anom_score_sequence.append(ood_scores)\n",
    "            for i in range(len(ood_target)):\n",
    "                # 1 indicates \"anomaly\"\n",
    "                anom_labels.append(1.)\n",
    "\n",
    "    anom_scores = torch.hstack(anom_score_sequence).cpu().numpy()\n",
    "    anom_labels = np.asarray(anom_labels)\n",
    "    \n",
    "    AUROC = roc_auc_score(anom_labels, anom_scores)\n",
    "\n",
    "    accuracy = 100. * correct / len(id_loader.dataset)\n",
    "    print('Test set: Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        correct, len(id_loader.dataset), accuracy))\n",
    "    print('Test Set: AUROC: {}\\n'.format(AUROC))\n",
    "\n",
    "    return accuracy, AUROC\n",
    "\n",
    "\n",
    "# TODO: Verify Done\n",
    "def test_ce_ks(model, id_loader, ood_loader, device):\n",
    "    # For KS, do confusion matrix\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    anomaly_index = 10\n",
    "    anom_pred = []\n",
    "    anom_labels = []\n",
    "    anom_score_sequence = []\n",
    "    pred_sequence = []\n",
    "    target_sequence = []\n",
    "    with torch.no_grad(): \n",
    "        for batch_idx, (id_data, id_target) in enumerate(id_loader):\n",
    "            id_data, id_target   = id_data.to(device), id_target.to(device)\n",
    "            id_output, _  = model(id_data)\n",
    "\n",
    "            # Compute number of correctly classified id instances\n",
    "            id_pred   = id_output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            _, id_idx = id_output.max(dim=1)\n",
    "            correct += (id_idx == id_target).sum().item()\n",
    "            id_anom_pred = [1. if id_pred[i] == anomaly_index else 0. for i in range(len(id_pred))]\n",
    "\n",
    "            # Concatenate the list (order matters here)\n",
    "            id_batch_anom_pred = id_anom_pred\n",
    "            anom_pred = anom_pred + id_batch_anom_pred\n",
    "\n",
    "            pred_sequence.append(id_pred)\n",
    "            target_sequence.append(id_target)\n",
    "\n",
    "            # Compute anomaly scores\n",
    "            id_scores = id_output[:,anomaly_index]\n",
    "            anom_score_sequence.append(id_scores)\n",
    "            for i in range(len(id_target)):\n",
    "                # 0 indicates \"nominal\"\n",
    "                anom_labels.append(0.)\n",
    "\n",
    "        for batch_idx, (ood_data, ood_target) in enumerate(ood_loader):\n",
    "            ood_target = anomaly_index * torch.ones_like(ood_target)\n",
    "            ood_data, ood_target = ood_data.to(device), ood_target.to(device)\n",
    "            ood_output, _ = model(ood_data)\n",
    "            ood_pred   = ood_output.argmax(dim=1, keepdim=True)\n",
    "            ood_anom_pred = [1. if ood_pred[i] == anomaly_index else 0. for i in range(len(ood_pred))]\n",
    "            ood_batch_anom_pred = ood_anom_pred\n",
    "            anom_pred = anom_pred + ood_batch_anom_pred\n",
    "            pred_sequence.append(ood_pred)\n",
    "            target_sequence.append(ood_target)\n",
    "            \n",
    "            # Compute anomaly scores\n",
    "            ood_scores = ood_output[:,anomaly_index]\n",
    "            anom_score_sequence.append(ood_scores)\n",
    "            for i in range(len(ood_target)):\n",
    "                # 1 indicates \"anomaly\"\n",
    "                anom_labels.append(1.)\n",
    "\n",
    "\n",
    "    anom_scores = torch.hstack(anom_score_sequence).cpu().numpy()\n",
    "    anom_labels = np.asarray(anom_labels)\n",
    "    anom_pred = np.asarray(anom_pred)\n",
    "    pred = torch.vstack(pred_sequence).cpu().numpy()\n",
    "    pred = np.ndarray.flatten(pred)\n",
    "    targets = torch.hstack(target_sequence).cpu().numpy()\n",
    "\n",
    "    # skl_conf_matrix = confusion_matrix(anom_labels, anom_pred)\n",
    "    # tn_count = skl_conf_matrix[0,0]\n",
    "    # fp_count = skl_conf_matrix[0,1]\n",
    "    # fn_count = skl_conf_matrix[1,0]\n",
    "    # tp_count = skl_conf_matrix[1,1]\n",
    "    # wandb.log({\"Eval True Negatives per Epoch\": tn_count}, step=epoch)\n",
    "    # wandb.log({\"Eval False Positives per Epoch\": fp_count}, step=epoch)\n",
    "    # wandb.log({\"Eval False Negatives per Epoch\": fn_count}, step=epoch)\n",
    "    # wandb.log({\"Eval True Positives per Epoch\": tp_count}, step=epoch)\n",
    "    # detection_conf_matrix = wandb.plot.confusion_matrix(y_true=anom_labels, preds=anom_pred)\n",
    "    # wandb.log({\"Detection Confusion Matrix\": detection_conf_matrix}, step=epoch)\n",
    "\n",
    "    # conf_matrix = wandb.plot.confusion_matrix(y_true=targets, preds=pred)\n",
    "    # wandb.log({\"Confusion Matrix\": conf_matrix}, step=epoch)\n",
    "\n",
    "    AUROC = roc_auc_score(anom_labels, anom_scores)\n",
    "\n",
    "    accuracy = 100. * correct / len(id_loader.dataset)\n",
    "    print('Test set: Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        correct, len(id_loader.dataset), accuracy))\n",
    "    print('Test Set: AUROC: {}\\n'.format(AUROC))\n",
    "\n",
    "    return accuracy, AUROC\n",
    "\n",
    "\n",
    "# TODO: Verify Correct\n",
    "def test_lm_ks(model, id_loader, ood_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    anomaly_index = 10\n",
    "    anom_pred = []\n",
    "    anom_labels = []\n",
    "    anom_score_sequence = []\n",
    "    pred_sequence = []\n",
    "    target_sequence = []\n",
    "    with torch.no_grad(): \n",
    "        for batch_idx, (id_data, id_target) in enumerate(id_loader):\n",
    "            id_data, id_target   = id_data.to(device), id_target.to(device)\n",
    "            id_output, _  = model(id_data)\n",
    "            \n",
    "            # Compute number of correctly classified id instances\n",
    "            id_pred   = id_output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            _, id_idx = id_output.max(dim=1)\n",
    "            correct += (id_idx == id_target).sum().item()\n",
    "            id_anom_pred = [1. if id_pred[i] == anomaly_index else 0. for i in range(len(id_pred))]\n",
    "\n",
    "            # Concatenate the list (order matters here)\n",
    "            batch_anom_pred = id_anom_pred ###ood_anom_pred + id_anom_pred\n",
    "            anom_pred = anom_pred + batch_anom_pred\n",
    "\n",
    "            pred_sequence.append(id_pred)\n",
    "            target_sequence.append(id_target)\n",
    "\n",
    "            id_scores = id_output[:,anomaly_index]\n",
    "            anom_score_sequence.append(id_scores)\n",
    "            for i in range(len(id_target)):\n",
    "                # 0 indicates \"nominal\"\n",
    "                anom_labels.append(0.)\n",
    "\n",
    "        for batch_idx, (ood_data, ood_target) in enumerate(ood_loader):\n",
    "            ood_target = anomaly_index * torch.ones_like(ood_target)\n",
    "            ood_data, ood_target = ood_data.to(device), ood_target.to(device)\n",
    "            ood_output, _ = model(ood_data)\n",
    "\n",
    "            ood_pred   = ood_output.argmax(dim=1, keepdim=True)\n",
    "            ood_anom_pred = [1. if ood_pred[i] == anomaly_index else 0. for i in range(len(ood_pred))]\n",
    "            pred_sequence.append(ood_pred)\n",
    "            target_sequence.append(ood_target)\n",
    "\n",
    "            # Compute anomaly scores\n",
    "            ood_scores = ood_output[:,anomaly_index]\n",
    "            anom_score_sequence.append(ood_scores)\n",
    "            for i in range(len(ood_target)):\n",
    "                # 1 indicates \"anomaly\"\n",
    "                anom_labels.append(1.)\n",
    "\n",
    "    anom_scores = torch.hstack(anom_score_sequence).cpu().numpy()\n",
    "    anom_labels = np.asarray(anom_labels)\n",
    "    anom_pred = np.asarray(anom_pred)\n",
    "    pred = torch.vstack(pred_sequence).cpu().numpy()\n",
    "    pred = np.ndarray.flatten(pred)\n",
    "    targets = torch.hstack(target_sequence).cpu().numpy()\n",
    "\n",
    "    # names = [\"nominal\", \"anomaly\"]\n",
    "    # skl_conf_matrix = confusion_matrix(anom_labels, anom_pred)\n",
    "    # tn_count = skl_conf_matrix[0,0]\n",
    "    # fp_count = skl_conf_matrix[0,1]\n",
    "    # fn_count = skl_conf_matrix[1,0]\n",
    "    # tp_count = skl_conf_matrix[1,1]\n",
    "    # wandb.log({\"Eval True Negatives per Epoch\": tn_count}, step=epoch)\n",
    "    # wandb.log({\"Eval False Positives per Epoch\": fp_count}, step=epoch)\n",
    "    # wandb.log({\"Eval False Negatives per Epoch\": fn_count}, step=epoch)\n",
    "    # wandb.log({\"Eval True Positives per Epoch\": tp_count}, step=epoch)\n",
    "    # detection_conf_matrix = wandb.plot.confusion_matrix(y_true=anom_labels, preds=anom_pred, class_names=names)\n",
    "    # wandb.log({\"Detection Confusion Matrix\": detection_conf_matrix}, step=epoch)\n",
    "\n",
    "    # conf_matrix = wandb.plot.confusion_matrix(y_true=targets, preds=pred)\n",
    "    # wandb.log({\"Confusion Matrix\": conf_matrix}, step=epoch)\n",
    "\n",
    "    AUROC = roc_auc_score(anom_labels, anom_scores)\n",
    "\n",
    "    accuracy = 100. * correct / len(id_loader.dataset)\n",
    "    print('Test set: Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        correct, len(id_loader.dataset), accuracy))\n",
    "    print('Test Set: AUROC: {}\\n'.format(AUROC))\n",
    "\n",
    "    return accuracy, AUROC\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start testing, fill out form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE_PATH = '/nfs/stak/users/noelt/Documents/Project/noelt_masters_project/models/' \n",
    "\n",
    "###########################################################################################\n",
    "# BEGIN EXPERIMENT FORM\n",
    "###########################################################################################\n",
    "\n",
    "#########################\n",
    "# Experiment Parameters #\n",
    "#########################\n",
    "\n",
    "batch_size=256\n",
    "\n",
    "# \"LS\" or \"KS\"\n",
    "detection_type = \"LS\"\n",
    "\n",
    "# \"CE\" or \"margin\"\n",
    "loss = \"CE\"\n",
    "\n",
    "# 0 through 4\n",
    "split = 0\n",
    "\n",
    "best_epoch = 60\n",
    "\n",
    "oe_test = True\n",
    "\n",
    "##########################\n",
    "# !!! VERY IMPORTANT !!! #\n",
    "##########################\n",
    "# If true, trumps loss and detection_type\n",
    "baseline = True\n",
    "\n",
    "# \"CE\" or \"margin\"; only relevant if baseline is True\n",
    "baseline_type = \"margin\"\n",
    "\n",
    "###########################################################################################\n",
    "# END EXPERIMENT FORM\n",
    "###########################################################################################\n",
    "\n",
    "num_classes = 10\n",
    "if detection_type == \"KS\":\n",
    "    num_classes = 11\n",
    "\n",
    "if baseline:\n",
    "    prefix = \"baseline\" if baseline_type == \"CE\" else \"baseline_margin\"\n",
    "else:\n",
    "    prefix = \"{}_{}\".format(loss, detection_type)\n",
    "\n",
    "directory = 'val_{}_new_class_test/epoch_{}_split_{}.pth'.format(prefix, best_epoch, split)\n",
    "PATH = BASE_PATH + directory\n",
    "\n",
    "# Right now, hardcoded to use CIFAR-100, with hardcoded splits\n",
    "# defined and generated in random_split_generator.py\n",
    "id_data, ood_data = build_split_datasets(split, oe_test)\n",
    "\n",
    "id_train_data, id_val_data, id_test_data    = id_data\n",
    "ood_train_data, ood_val_data, ood_test_data = ood_data\n",
    "\n",
    "# Constructing Dataloaders\n",
    "id_train_loader = data.DataLoader(id_train_data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "id_val_loader   = data.DataLoader(id_val_data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "id_test_loader   = data.DataLoader(id_test_data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "ood_train_loader   = data.DataLoader(ood_train_data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "ood_val_loader   = data.DataLoader(ood_val_data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "ood_test_loader   = data.DataLoader(ood_test_data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "model = FeatureExtractor(\"efficientnet_b1\", num_classes)\n",
    "model.load_state_dict(torch.load(PATH)['model_state_dict'])\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "top_k=1\n",
    "\n",
    "if baseline:\n",
    "    if baseline_type == \"CE\":\n",
    "        acc, auc = test_ce_ls(model, id_test_loader, ood_test_loader, device)\n",
    "    else:\n",
    "        acc, margin_auc, max_logit_auc = test_lm_ls(model, top_k, id_test_loader, ood_test_loader, device)\n",
    "if detection_type == \"LS\" and loss == \"margin\":\n",
    "    acc, margin_auc, max_logit_auc = test_lm_ls(model, top_k, id_test_loader, ood_test_loader, device)\n",
    "elif detection_type == \"LS\" and loss == \"CE\":\n",
    "    acc, auc = test_ce_ls(model, id_test_loader, ood_test_loader, device)\n",
    "elif detection_type == \"KS\" and loss == \"margin\":\n",
    "    acc, auc = test_lm_ks(model, id_test_loader, ood_test_loader, device)\n",
    "elif detection_type == \"KS\" and loss == \"CE\":\n",
    "    acc, auc = test_ce_ks(model, id_test_loader, ood_test_loader, device)\n",
    "\n",
    "print(\"Accuracy: {}\".format(acc))\n",
    "\n",
    "if detection_type == \"LS\" and loss == \"margin\":\n",
    "    print(\"Margin AUROC: {}\".format(margin_auc))\n",
    "    print(\"Max Logit AUROC: {}\".format(max_logit_auc))\n",
    "else:\n",
    "    print(\"AUROC: {}\".format(auc))\n",
    "\n",
    "if baseline:\n",
    "    print(\"{}baseline experiment on Split {}, from epoch {}, {} test, Done\".format(\"\" if baseline_type == \"CE\" else \"margin \", split, best_epoch, \"OE\" if oe_test else \"new class\"))\n",
    "else:\n",
    "    print(\"{} + {} Experiment on Split {}, from epoch {}, {} test, Done\".format(detection_type, loss, split, best_epoch, \"OE\" if oe_test else \"new class\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 ('project': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3add45fc69f05cc29cf9d44865c28a2c1800260c2b05417b30b97f2c4a51861"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
