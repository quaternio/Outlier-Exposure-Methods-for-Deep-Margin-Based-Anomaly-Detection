{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Of the ID classes (which form a single high-level partition), we need 3 subsets:\n",
    "    ID1) ID classes\n",
    "        a) Training ID\n",
    "        b) Validation ID\n",
    "        c) Test ID\n",
    "\n",
    "Of the OOD classes, we have 3 high-level partitions:\n",
    "    OOD1) Training OOD Classes (OE data)\n",
    "    OOD2) Validation OOD Classes\n",
    "    OOD3) Test OOD Classes\n",
    "\n",
    "Question: Should training OOD classes show up in validation set or test set?\n",
    "\n",
    "Answer:   No. I don't think they should. At each phase, we want to present the network with previously unseen anomaly types.\n",
    "          Because of this, we can create 3 monolithic datasets from OOD classes.\n",
    "\n",
    "To brainstorm a bit... \n",
    "    1) We could take CIFAR-100 and split it into its train and test parts\n",
    "    2) For both parts, build all four partitions. You should have 8 partitions in total.\n",
    "    3) Two of these partitions will be ID partitions: Raw Training ID and Test ID\n",
    "        a) Partition Raw Training ID into an 80/20 split. Call the 80% partition the Training ID partition and\n",
    "           the 20% partition the Validation ID partition. At this point, you will have 9 total partitions; three of \n",
    "           which will be for ID data.\n",
    "    4) You still have 6 OOD partitions... what do we do with these? Throw out the Training OOD class \"test\" set. This will create a more \n",
    "       class-balanced training process. Concatenate the sets corresponding to validation OOD classes and to test OOD classes. \n",
    "       You should now have 6 partitions.\n",
    "    5) Proceed!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import pickle as pkl\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "import torchutil.data\n",
    "import torchutil\n",
    "from adtools.data.split import ClassSplit\n",
    "from random_split_generator import FourWayClassSplit\n",
    "\n",
    "with open('id-ood-splits/split0.pkl', 'rb') as f:\n",
    "    id_ood_split = pkl.load(f) # Type is adtools.data.FourWayClassSplit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "# Wrapper around torchvision.datasets.CIFAR100 which provides an interface for\n",
    "# getting image labels without loading images from disk (significantly speeds\n",
    "# up ID/OOD splitting process)\n",
    "train_dataset = torchutil.data.CIFAR100(root = 'data', train = True, download = True)\n",
    "test_dataset = torchutil.data.CIFAR100(root = 'data', train = False, download = True)\n",
    "\n",
    "id_train_dataset, ood_train_dataset, ood_val1_dataset, ood_test1_dataset = id_ood_split.split_dataset(train_dataset)\n",
    "id_test_dataset, _, ood_val2_dataset, ood_test2_dataset = id_ood_split.split_dataset(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Pick up here!\n",
    "\n",
    "# Map ID labels and OOD labels to [0, 1, 2, ...]\n",
    "id_train_dataset = torchutil.data.TransformingDataset(\n",
    "    id_train_dataset,\n",
    "    target_transform = torchutil.data.LabelMappingTransform(\n",
    "        label_list = id_ood_split.id_labels()\n",
    "    )\n",
    ")\n",
    "ood_train_dataset = torchutil.data.TransformingDataset(\n",
    "    ood_train_dataset,\n",
    "    target_transform = torchutil.data.LabelMappingTransform(\n",
    "        label_list = id_ood_split.ood_train_labels()\n",
    "    )\n",
    ")\n",
    "\n",
    "ood_val1_dataset = torchutil.data.TransformingDataset(\n",
    "    ood_val1_dataset,\n",
    "    target_transform = torchutil.data.LabelMappingTransform(\n",
    "        label_list = id_ood_split.ood_val_labels()\n",
    "    )\n",
    ")\n",
    "\n",
    "ood_test1_dataset = torchutil.data.TransformingDataset(\n",
    "    ood_test1_dataset,\n",
    "    target_transform = torchutil.data.LabelMappingTransform(\n",
    "        label_list = id_ood_split.ood_test_labels()\n",
    "    )\n",
    ")\n",
    "\n",
    "id_test_dataset = torchutil.data.TransformingDataset(\n",
    "    id_test_dataset,\n",
    "    target_transform = torchutil.data.LabelMappingTransform(\n",
    "        label_list = id_ood_split.id_labels()\n",
    "    )\n",
    ")\n",
    "\n",
    "ood_val2_dataset = torchutil.data.TransformingDataset(\n",
    "    ood_val2_dataset,\n",
    "    target_transform = torchutil.data.LabelMappingTransform(\n",
    "        label_list = id_ood_split.ood_val_labels()\n",
    "    )\n",
    ")\n",
    "\n",
    "ood_test2_dataset = torchutil.data.TransformingDataset(\n",
    "    ood_test2_dataset,\n",
    "    target_transform = torchutil.data.LabelMappingTransform(\n",
    "        label_list = id_ood_split.ood_test_labels()\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "ood_val_dataset  = ConcatDataset([ood_val1_dataset, ood_val2_dataset])\n",
    "ood_test_dataset = ConcatDataset([ood_test1_dataset, ood_test2_dataset])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At this point, all of the datasets have been successfully constructed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3add45fc69f05cc29cf9d44865c28a2c1800260c2b05417b30b97f2c4a51861"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 ('project': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
