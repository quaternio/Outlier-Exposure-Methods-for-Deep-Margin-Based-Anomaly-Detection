{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n",
      "id batch 0 starting\n",
      "id batch 1 starting\n",
      "id batch 2 starting\n",
      "id batch 3 starting\n",
      "ood batch 0 starting\n",
      "ood batch 1 starting\n",
      "ood batch 2 starting\n",
      "ood batch 3 starting\n",
      "ood batch 4 starting\n",
      "ood batch 5 starting\n",
      "ood batch 6 starting\n",
      "ood batch 7 starting\n",
      "ood batch 8 starting\n",
      "ood batch 9 starting\n",
      "ood batch 10 starting\n",
      "ood batch 11 starting\n",
      "ood batch 12 starting\n",
      "ood batch 13 starting\n",
      "ood batch 14 starting\n",
      "ood batch 15 starting\n",
      "ood batch 16 starting\n",
      "ood batch 17 starting\n",
      "ood batch 18 starting\n",
      "ood batch 19 starting\n",
      "ood batch 20 starting\n",
      "ood batch 21 starting\n",
      "ood batch 22 starting\n",
      "ood batch 23 starting\n",
      "Test set: Accuracy: 649/1000 (65%)\n",
      "Test Set: AUROC: 0.5688960000000001\n",
      "\n",
      "Accuracy: 64.9\n",
      "AUROC: 0.5688960000000001\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cuda:0\")\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "import copy\n",
    "from torchvision.models import resnet18, resnet34, resnet50, efficientnet_b1\n",
    "from torch.utils import data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, RMSprop\n",
    "import numpy as np\n",
    "from large_margin import LargeMarginLoss\n",
    "import time\n",
    "import argparse\n",
    "from data import build_split_datasets\n",
    "import pickle as pkl\n",
    "from random_split_generator import FourWayClassSplit\n",
    "from torch.utils.data import ConcatDataset\n",
    "import datetime\n",
    "from experiment import FeatureExtractor\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import pandas as pd\n",
    "from large_margin import _get_grad\n",
    "\n",
    "def test_distance(logits, features, device):\n",
    "    eps = 1e-8\n",
    "\n",
    "    entries = 100\n",
    "    # print(\"logits: {}\".format(logits))\n",
    "    # pd_logits = pd.DataFrame(logits.detach().cpu().numpy())\n",
    "    prob = F.softmax(logits, dim=1)\n",
    "    # print(\"prob: {}\".format(prob))\n",
    "    # pd_prob = pd.DataFrame(prob.detach().cpu().numpy())\n",
    "\n",
    "    max_indices = torch.argmax(prob, dim=1)\n",
    "\n",
    "    pseudo_correct_prob, _ = torch.max(prob, dim=1, keepdim=True)\n",
    "    # print(\"pseudo_correct_prob: {}\".format(pseudo_correct_prob))\n",
    "    # print(\"pseudo_correct_prob shape: {}\".format(pseudo_correct_prob.shape))\n",
    "\n",
    "    pseudo_other_prob = torch.zeros(prob.shape).to(device)\n",
    "    pseudo_other_prob.copy_(prob)\n",
    "    pseudo_other_prob[torch.arange(prob.shape[0]),max_indices] = 0.\n",
    "\n",
    "    # Grabs the next most likely class probabilities\n",
    "    if top_k > 1:\n",
    "        topk_prob, _ = pseudo_other_prob.topk(top_k, dim=1)\n",
    "    else:\n",
    "        topk_prob, _ = pseudo_other_prob.max(dim=1, keepdim=True)\n",
    "\n",
    "    # print(\"pseudo correct prob shape: {}\".format(pseudo_correct_prob.shape))\n",
    "    # print(\"topk_prob shape: {}\".format(topk_prob.shape))\n",
    "\n",
    "    pseudo_diff_prob = pseudo_correct_prob - topk_prob\n",
    "\n",
    "    for i, feature_map in enumerate(features):\n",
    "        if i == len(features)-1:\n",
    "            diff_grad = torch.stack([_get_grad(pseudo_diff_prob[:, i], feature_map) for i in range(top_k)],\n",
    "                                dim=1)\n",
    "            diff_gradnorm = torch.norm(diff_grad, p=2, dim=2)\n",
    "            diff_gradnorm.detach_()\n",
    "            distance = pseudo_diff_prob / (diff_gradnorm + eps)\n",
    "\n",
    "    return distance\n",
    "\n",
    "\n",
    "# TODO: Check that ID margins are larger than OOD margins; this is an empirical sanity check\n",
    "def sanity_test_lm_ls(model, top_k, id_loader, ood_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    anomaly_index = 10\n",
    "    num_classes = 10\n",
    "    anom_pred = []\n",
    "    anom_labels = []\n",
    "    anom_score_sequence = []\n",
    "    pred_sequence = []\n",
    "    target_sequence = []\n",
    "    \n",
    "    for batch_idx, ((id_data, id_target), (ood_data, ood_target)) in enumerate(zip(id_loader, ood_loader)):\n",
    "        ood_target = anomaly_index * torch.ones_like(ood_target)\n",
    "\n",
    "        id_one_hot = torch.zeros(len(id_target), num_classes).scatter_(1, id_target.unsqueeze(1), 1.).float()\n",
    "        ood_one_hot = (1/id_one_hot.shape[1])*torch.ones((len(ood_target), id_one_hot.shape[1])).to(device)\n",
    "        id_one_hot, ood_one_hot = id_one_hot.to(device), ood_one_hot.to(device)\n",
    "\n",
    "        print(\"id_one_hot: {}\".format(id_one_hot))\n",
    "        print(\"ood_one_hot: {}\".format(ood_one_hot))\n",
    "\n",
    "        id_data, id_target   = id_data.to(device), id_target.to(device)\n",
    "        ood_data, ood_target = ood_data.to(device), ood_target.to(device)\n",
    "\n",
    "        model.clear_features()\n",
    "        id_data = id_data.to(device)\n",
    "        id_output, id_features  = model(id_data)\n",
    "        for id_feature in id_features:\n",
    "            id_feature.retain_grad()\n",
    "        #id_features = copy.deepcopy(id_features)\n",
    "        print(\"entering ID margin comp\")\n",
    "\n",
    "        # lm(id_output, id_one_hot, id_features)\n",
    "        # raw_id_distance = lm.get_discriminant()\n",
    "        # id_distance = torch.abs(raw_id_distance)\n",
    "    \n",
    "        print(\"raw id logits: {}\".format(id_output[0,:]))\n",
    "\n",
    "        ###########################\n",
    "        # ID Distance Computation #\n",
    "        ###########################\n",
    "        raw_id_distance = test_distance(id_output, id_features, device)\n",
    "        id_distance = torch.abs(raw_id_distance)\n",
    "\n",
    "\n",
    "        # FOR DEBUG\n",
    "        # pd_id_logits.to_csv('id_logit.csv', header=False, mode='a+')\n",
    "        # pd_id_prob.to_csv('id_prob.csv', header=False, mode='a+')\n",
    "        # pd_id_distance = pd.DataFrame(raw_id_distance.detach().cpu().numpy())\n",
    "        # pd_id_distance.to_csv('id_distance.csv', header=False, mode='a+')\n",
    "\n",
    "\n",
    "        print(\"raw id distance: {}\".format(id_distance)) #[0,:]))\n",
    "\n",
    "        model.clear_features()\n",
    "        ood_output, ood_features = model(ood_data)\n",
    "        for ood_feature in ood_features:\n",
    "            ood_feature.retain_grad()\n",
    "        #ood_features = copy.deepcopy(ood_features)\n",
    "        print(\"entering OOD margin comp\")\n",
    "\n",
    "        # lm(ood_output, ood_one_hot, ood_features)\n",
    "        # raw_ood_distance = lm.get_discriminant()\n",
    "        # ood_distance = torch.abs(raw_ood_distance)\n",
    "\n",
    "        print(\"raw ood logits: {}\".format(ood_output[0,:]))\n",
    "\n",
    "        ############################\n",
    "        # OOD Distance Computation #\n",
    "        ############################\n",
    "        raw_ood_distance = test_distance(ood_output, ood_features, device)\n",
    "        ood_distance = torch.abs(raw_ood_distance)\n",
    "\n",
    "\n",
    "        # FOR DEBUG\n",
    "        # pd_ood_logits.to_csv('ood_logit.csv', header=False, mode='a+')\n",
    "        # pd_ood_prob.to_csv('ood_prob.csv', header=False, mode='a+')\n",
    "        # pd_ood_distance = pd.DataFrame(raw_ood_distance.detach().cpu().numpy())\n",
    "        # pd_ood_distance.to_csv('ood_distance.csv', header=False, mode='a+')\n",
    "\n",
    "\n",
    "        # print(\"id distance (should be same as above): {}\".format(id_distance))\n",
    "        print(\"raw ood distance: {}\".format(ood_distance)) #[0,:]))\n",
    "\n",
    "        ood_pred   = ood_output.argmax(dim=1, keepdim=True)\n",
    "        ood_anom_pred = [1. if ood_pred[i] == anomaly_index else 0. for i in range(len(ood_pred))]\n",
    "\n",
    "        # Compute number of correctly classified id instances\n",
    "        id_pred   = id_output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "        _, id_idx = id_output.max(dim=1)\n",
    "        correct += (id_idx == id_target).sum().item()\n",
    "        id_anom_pred = [1. if id_pred[i] == anomaly_index else 0. for i in range(len(id_pred))]\n",
    "\n",
    "        # Concatenate the list (order matters here)\n",
    "        batch_anom_pred = ood_anom_pred + id_anom_pred\n",
    "        anom_pred = anom_pred + batch_anom_pred\n",
    "\n",
    "        pred_sequence.append(ood_pred)\n",
    "        pred_sequence.append(id_pred)\n",
    "\n",
    "        target_sequence.append(ood_target)\n",
    "        target_sequence.append(id_target)\n",
    "\n",
    "        # Compute anomaly scores\n",
    "        # # Use discriminant (distance) function to compute ood_scores\n",
    "        # ood_distance = torch.abs(ood_distance)\n",
    "        ood_scores, _ = torch.max(-1 * ood_distance, dim=1)\n",
    "        ## print(ood_distance[:,1:].shape)\n",
    "        # ood_scores = torch.norm(ood_distance, p=2, dim=1) #[:,1:], p=2, dim=1)\n",
    "        # print('ood scores: {}'.format(ood_scores))\n",
    "        anom_score_sequence.append(ood_scores)\n",
    "        for i in range(len(ood_target)):\n",
    "            # 1 indicates \"anomaly\"\n",
    "            anom_labels.append(1.)\n",
    "\n",
    "        # Use discriminant function to compute id_scores\n",
    "        # id_distance = torch.abs(id_distance)\n",
    "        id_scores, _ = torch.max(-1 * id_distance, dim=1)\n",
    "        ## print(id_distance[:,1:].shape)\n",
    "        # id_scores = torch.norm(id_distance, p=2, dim=1) #[:,1:], p=2, dim=1)\n",
    "        # print('id scores: {}'.format(id_scores))\n",
    "        anom_score_sequence.append(id_scores)\n",
    "        for i in range(len(id_target)):\n",
    "            # 0 indicates \"nominal\"\n",
    "            anom_labels.append(0.)\n",
    "\n",
    "    anom_scores = torch.hstack(anom_score_sequence).cpu().detach().numpy()\n",
    "    anom_labels = np.asarray(anom_labels)\n",
    "    anom_pred = np.asarray(anom_pred)\n",
    "    pred = torch.vstack(pred_sequence).cpu().detach().numpy()\n",
    "    pred = np.ndarray.flatten(pred)\n",
    "    targets = torch.hstack(target_sequence).cpu().detach().numpy()\n",
    "\n",
    "    AUROC = roc_auc_score(anom_labels, anom_scores)\n",
    "\n",
    "    accuracy = 100. * correct / len(id_loader.dataset)\n",
    "    print('Test set: Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        correct, len(id_loader.dataset), accuracy))\n",
    "    print('Test Set: AUROC: {}\\n'.format(AUROC))\n",
    "\n",
    "    return accuracy, AUROC, id_distance, ood_distance\n",
    "\n",
    "\n",
    "def split_test_lm_ls(model, top_k, id_loader, ood_loader, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    anomaly_index = 10\n",
    "    num_classes = 10\n",
    "    anom_pred = []\n",
    "    anom_labels = []\n",
    "    anom_score_sequence = []\n",
    "    pred_sequence = []\n",
    "    target_sequence = []\n",
    "    \n",
    "    for batch_idx, (id_data, id_target) in enumerate(id_loader):\n",
    "        print(\"id batch {} starting\".format(batch_idx))\n",
    "        id_one_hot = torch.zeros(len(id_target), num_classes).scatter_(1, id_target.unsqueeze(1), 1.).float()\n",
    "        id_one_hot = id_one_hot.to(device)\n",
    "        #print(\"id_one_hot: {}\".format(id_one_hot))\n",
    "        id_data, id_target   = id_data.to(device), id_target.to(device)\n",
    "        model.clear_features()\n",
    "        id_data = id_data.to(device)\n",
    "        id_output, id_features  = model(id_data)\n",
    "        for id_feature in id_features:\n",
    "            id_feature.retain_grad()\n",
    "\n",
    "        ###########################\n",
    "        # ID Distance Computation #\n",
    "        ###########################\n",
    "        raw_id_distance = test_distance(id_output, id_features, device)\n",
    "        id_distance = torch.abs(raw_id_distance)\n",
    "\n",
    "        # # FOR DEBUG\n",
    "        # pd_id_logits = pd.DataFrame(id_output.detach().cpu().numpy())\n",
    "        # pd_id_logits.to_csv('id_logit.csv', header=False, mode='a+')\n",
    "        # pd_id_distance = pd.DataFrame(raw_id_distance.detach().cpu().numpy())\n",
    "        # pd_id_distance.to_csv('id_distance.csv', header=False, mode='a+')\n",
    "\n",
    "        ## print(\"raw id distance: {}\".format(id_distance)) #[0,:]))\n",
    "\n",
    "        # Compute number of correctly classified id instances\n",
    "        id_pred   = id_output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "        _, id_idx = id_output.max(dim=1)\n",
    "        correct += (id_idx == id_target).sum().item()\n",
    "        id_anom_pred = [1. if id_pred[i] == anomaly_index else 0. for i in range(len(id_pred))]\n",
    "\n",
    "        # Concatenate the list (order matters here)\n",
    "        id_batch_anom_pred = id_anom_pred\n",
    "        anom_pred = anom_pred + id_batch_anom_pred\n",
    "        pred_sequence.append(id_pred)\n",
    "        target_sequence.append(id_target)\n",
    "\n",
    "        # Compute anomaly scores\n",
    "        # Use discriminant function to compute id_scores\n",
    "        # id_distance = torch.abs(id_distance)\n",
    "        id_scores, _ = torch.max(-1 * id_distance, dim=1)\n",
    "\n",
    "        # Detaching is important here because it removes these scores from the computational graph\n",
    "        anom_score_sequence.append(id_scores.detach().cpu())\n",
    "        for i in range(len(id_target)):\n",
    "            # 0 indicates \"nominal\"\n",
    "            anom_labels.append(0.)\n",
    "\n",
    "    for batch_idx, (ood_data, ood_target) in enumerate(ood_loader):\n",
    "        print(\"ood batch {} starting\".format(batch_idx))\n",
    "        ood_target = anomaly_index * torch.ones_like(ood_target)\n",
    "        ood_one_hot = (1/id_one_hot.shape[1])*torch.ones((len(ood_target), id_one_hot.shape[1])).to(device)\n",
    "        ood_one_hot = ood_one_hot.to(device)\n",
    "        #print(\"ood_one_hot: {}\".format(ood_one_hot))\n",
    "        ood_data, ood_target = ood_data.to(device), ood_target.to(device)\n",
    "        model.clear_features()\n",
    "        ood_output, ood_features = model(ood_data)\n",
    "        for ood_feature in ood_features:\n",
    "            ood_feature.retain_grad()\n",
    "        ##print(\"entering OOD margin comp\")\n",
    "        ## print(\"raw ood logits: {}\".format(ood_output[0,:]))\n",
    "\n",
    "        ############################\n",
    "        # OOD Distance Computation #\n",
    "        ############################\n",
    "        raw_ood_distance = test_distance(ood_output, ood_features, device)\n",
    "        ood_distance = torch.abs(raw_ood_distance)\n",
    "\n",
    "        # # FOR DEBUG\n",
    "        # pd_ood_logits = pd.DataFrame(ood_output.detach().cpu().numpy())\n",
    "        # pd_ood_logits.to_csv('ood_logit.csv', header=False, mode='a+')\n",
    "        # pd_ood_distance = pd.DataFrame(raw_ood_distance.detach().cpu().numpy())\n",
    "        # pd_ood_distance.to_csv('ood_distance.csv', header=False, mode='a+')\n",
    "\n",
    "        ## print(\"raw ood distance: {}\".format(ood_distance)) #[0,:]))\n",
    "        ood_pred   = ood_output.argmax(dim=1, keepdim=True)\n",
    "        ood_anom_pred = [1. if ood_pred[i] == anomaly_index else 0. for i in range(len(ood_pred))]\n",
    "        \n",
    "        # Concatenate the list (order matters here)\n",
    "        ood_batch_anom_pred = ood_anom_pred\n",
    "        anom_pred = anom_pred + ood_batch_anom_pred\n",
    "        pred_sequence.append(ood_pred)\n",
    "        target_sequence.append(ood_target)\n",
    "\n",
    "        ood_scores, _ = torch.max(-1 * ood_distance, dim=1)\n",
    "\n",
    "        # Detaching is important here because it removes these scores from computational graph\n",
    "        anom_score_sequence.append(ood_scores.detach().cpu())\n",
    "        for i in range(len(ood_target)):\n",
    "            # 1 indicates \"anomaly\"\n",
    "            anom_labels.append(1.)\n",
    "\n",
    "    anom_scores = torch.hstack(anom_score_sequence).cpu().detach().numpy()\n",
    "    anom_labels = np.asarray(anom_labels)\n",
    "    anom_pred = np.asarray(anom_pred)\n",
    "    pred = torch.vstack(pred_sequence).cpu().detach().numpy()\n",
    "    pred = np.ndarray.flatten(pred)\n",
    "    targets = torch.hstack(target_sequence).cpu().detach().numpy()\n",
    "\n",
    "    AUROC = roc_auc_score(anom_labels, anom_scores)\n",
    "\n",
    "    accuracy = 100. * correct / len(id_loader.dataset)\n",
    "    print('Test set: Accuracy: {}/{} ({:.0f}%)'.format(\n",
    "        correct, len(id_loader.dataset), accuracy))\n",
    "    print('Test Set: AUROC: {}\\n'.format(AUROC))\n",
    "\n",
    "    return accuracy, AUROC, id_distance, ood_distance\n",
    "\n",
    "\n",
    "PATH = '/nfs/stak/users/noelt/Documents/Project/noelt_masters_project/models/val_baseline_margin_new_class_test/epoch_259_split_0.pth'\n",
    "batch_size=256\n",
    "\n",
    "# Right now, hardcoded to use CIFAR-100, with hardcoded splits\n",
    "# defined and generated in random_split_generator.py\n",
    "id_data, ood_data = build_split_datasets(0, False)\n",
    "\n",
    "id_train_data, id_val_data, id_test_data    = id_data\n",
    "ood_train_data, ood_val_data, ood_test_data = ood_data\n",
    "\n",
    "# Constructing Dataloaders\n",
    "id_train_loader = data.DataLoader(id_train_data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "id_val_loader   = data.DataLoader(id_val_data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "id_test_loader   = data.DataLoader(id_test_data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "ood_train_loader   = data.DataLoader(ood_train_data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "ood_val_loader   = data.DataLoader(ood_val_data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "ood_test_loader   = data.DataLoader(ood_test_data, batch_size=batch_size, shuffle=True, drop_last=False)\n",
    "\n",
    "model = FeatureExtractor(\"efficientnet_b1\", 10)\n",
    "model.load_state_dict(torch.load(PATH)['model_state_dict'])\n",
    "model.eval()\n",
    "model = model.to(device)\n",
    "\n",
    "top_k=1\n",
    "\n",
    "# lm = LargeMarginLoss(\n",
    "#     gamma=19600,\n",
    "#     alpha_factor=7,\n",
    "#     top_k=top_k,\n",
    "#     dist_norm=2\n",
    "# )\n",
    "\n",
    "acc, auc, id_dist, ood_dist = split_test_lm_ls(model, top_k, id_val_loader, ood_val_loader, device)\n",
    "\n",
    "print(\"Accuracy: {}\".format(acc))\n",
    "print(\"AUROC: {}\".format(auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.8 ('project': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a3add45fc69f05cc29cf9d44865c28a2c1800260c2b05417b30b97f2c4a51861"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
