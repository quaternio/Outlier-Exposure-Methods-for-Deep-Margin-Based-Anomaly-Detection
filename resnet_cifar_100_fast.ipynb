{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mtnoel20\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "/nfs/stak/users/noelt/Documents/Project/noelt_masters_project/envs/project/lib64/python3.6/site-packages/IPython/html.py:14: ShimWarning: The `IPython.html` package has been deprecated since IPython 4.0. You should import from `notebook` instead. `IPython.html.widgets` has moved to `ipywidgets`.\n",
      "  \"`IPython.html.widgets` has moved to `ipywidgets`.\", ShimWarning)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.12.21"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/nfs/stak/users/noelt/Documents/Project/noelt_masters_project/wandb/run-20220708_111118-1l756ybv</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href=\"https://wandb.ai/tnoel20/Thomas-Masters-Project/runs/1l756ybv\" target=\"_blank\">pious-wildflower-61</a></strong> to <a href=\"https://wandb.ai/tnoel20/Thomas-Masters-Project\" target=\"_blank\">Weights & Biases</a> (<a href=\"https://wandb.me/run\" target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "\n",
    "import torch\n",
    "device = torch.device(\"cuda\")\n",
    "import torch.nn.functional as F\n",
    "import wandb\n",
    "from torchvision.models import resnet18, resnet34, resnet50, efficientnet_b1\n",
    "from torchvision.models.feature_extraction import get_graph_node_names\n",
    "from torchvision.models.feature_extraction import create_feature_extractor\n",
    "\n",
    "torch.manual_seed(0)\n",
    "\n",
    "# Setup Weights and Biases and specify hyperparameters\n",
    "wandb.init(project=\"Thomas-Masters-Project\")\n",
    "\n",
    "learning_rate = 0.001\n",
    "epochs = 5\n",
    "batch_size = 256\n",
    "net_type = \"pretrained_efficientnet_b1\"\n",
    "\n",
    "wandb.config = {\n",
    "    \"learning_rate\": learning_rate,\n",
    "    \"epochs\": epochs,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"network\": net_type\n",
    "}\n",
    "\n",
    "def test(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad(): \n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output, *_ = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            _, idx = output.max(dim=1)\n",
    "            correct += (idx == target).sum().item()\n",
    "\n",
    "            # Clear the computed features\n",
    "            model.clear_features()\n",
    "\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('Test set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(test_loader.dataset), accuracy))\n",
    "\n",
    "    wandb.log({\"accuracy\": accuracy})\n",
    "\n",
    "\n",
    "def test_ce(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    with torch.no_grad(): \n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            pred = output.argmax(dim=1, keepdim=True) # get the index of the max log-probability\n",
    "            _, idx = output.max(dim=1)\n",
    "            correct += (idx == target).sum().item()\n",
    "\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print('Test set: Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "        correct, len(test_loader.dataset), accuracy))\n",
    "\n",
    "    wandb.log({\"accuracy\": accuracy})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _init_resnet_18(output_size, pretrained = False, features_hook = None):\n",
    "    model = resnet18(pretrained=pretrained)\n",
    "    model.fc = torch.nn.Linear(512, output_size)\n",
    "    if features_hook is not None:\n",
    "        for name, module in model.named_modules():\n",
    "            if name in ['layer1', 'layer2', 'layer3', 'layer4']:\n",
    "                module.register_forward_hook(features_hook)\n",
    "\n",
    "    return model\n",
    "    \n",
    "def _init_resnet_34(output_size, pretrained = False, features_hook = None):\n",
    "    model = resnet34(pretrained=pretrained)\n",
    "    model.fc = torch.nn.Linear(1024, output_size)\n",
    "    if features_hook is not None:\n",
    "        for name, module in model.named_modules():\n",
    "            if name in ['layer1', 'layer2', 'layer3', 'layer4']:\n",
    "                module.register_forward_hook(features_hook)\n",
    "\n",
    "    return model\n",
    "\n",
    "def _init_resnet_50(output_size, pretrained = False, features_hook = None):\n",
    "    model = resnet50(pretrained=pretrained)\n",
    "    model.fc = torch.nn.Linear(2048, output_size)\n",
    "    if features_hook is not None:\n",
    "        for name, module in model.named_modules():\n",
    "            if name in ['layer1', 'layer2', 'layer3', 'layer4']:\n",
    "                module.register_forward_hook(features_hook)\n",
    "\n",
    "    return model\n",
    "\n",
    "def _init_efficientnet_b1(output_size, pretrained = False, features_hook = None):\n",
    "    model = efficientnet_b1(pretrained=pretrained)\n",
    "    model.classifier = torch.nn.Linear(1280, output_size)\n",
    "    if features_hook is not None:\n",
    "        for name, module in model.named_modules():\n",
    "            if name in ['features.0', 'features.1', 'features.2', 'features.3', 'features.4', 'features.5', 'features.6', 'features.7', 'features.8',]:\n",
    "                module.register_forward_hook(features_hook)\n",
    "\n",
    "    return model\n",
    "\n",
    "def create_pretrained_model(architecture, n_classes, features_hook = None):\n",
    "    pretrained = True\n",
    "    if 'resnet18' in architecture:\n",
    "        net = _init_resnet_18(n_classes, pretrained, features_hook)\n",
    "    elif 'resnet34' in architecture:\n",
    "        net = _init_resnet_34(n_classes, pretrained, features_hook)\n",
    "    elif 'resnet50' in architecture:\n",
    "        net = _init_resnet_50(n_classes, pretrained, features_hook)\n",
    "    elif 'efficientnet_b1' in architecture:\n",
    "        net = _init_efficientnet_b1(n_classes, pretrained, features_hook)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    return net\n",
    "    \n",
    "def create_model(architecture, n_classes, features_hook = None):\n",
    "    pretrained = False\n",
    "    if 'resnet18' in architecture:\n",
    "        net = _init_resnet_18(n_classes, pretrained, features_hook)\n",
    "    elif 'resnet34' in architecture:\n",
    "        net = _init_resnet_34(n_classes, pretrained, features_hook)\n",
    "    elif 'resnet50' in architecture:\n",
    "        net = _init_resnet_50(n_classes, pretrained, features_hook)\n",
    "    elif 'efficientnet_b1' in architecture:\n",
    "        net = _init_efficientnet_b1(n_classes, pretrained, features_hook)\n",
    "    else:\n",
    "        raise NotImplementedError()\n",
    "\n",
    "    return net\n",
    "\n",
    "class FeatureExtractor(torch.nn.Module):\n",
    "    def __init__(self, architecture, n_classes = None):\n",
    "        super().__init__()\n",
    "        self._features = []\n",
    "        if 'pretrained' in architecture:\n",
    "            self.model = create_pretrained_model(\n",
    "                architecture, \n",
    "                n_classes, \n",
    "                features_hook=self.feature_hook)\n",
    "        else:\n",
    "            self.model = create_model(\n",
    "                architecture, \n",
    "                n_classes, \n",
    "                features_hook=self.feature_hook)\n",
    "\n",
    "    def feature_hook(self, module, input, output):\n",
    "        self._features.append(output)\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.model(x)\n",
    "        return logits, self._features\n",
    "\n",
    "    def clear_features(self):\n",
    "        self._features = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torch.utils import data\n",
    "from torchvision import datasets\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam, SGD, RMSprop\n",
    "\n",
    "train_loader = data.DataLoader(\n",
    "        datasets.CIFAR100('./data', train=True, download=True,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229, 0.224, 0.225])\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "test_loader = data.DataLoader(\n",
    "        datasets.CIFAR100('./data', train=False,\n",
    "                       transform=transforms.Compose([\n",
    "                           transforms.ToTensor(),\n",
    "                           transforms.Normalize(mean=[0.485,0.456,0.406], std=[0.229, 0.224, 0.225])\n",
    "                       ])),\n",
    "        batch_size=batch_size, shuffle=False, drop_last=False)\n",
    "\n",
    "num_training_classes = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/efficientnet_b1_rwightman-533bc792.pth\" to /nfs/stak/users/noelt/.cache/torch/hub/checkpoints/efficientnet_b1_rwightman-533bc792.pth\n",
      "77.8%IOPub message rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_msg_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_msg_rate_limit=1000.0 (msgs/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 0 [0/50000 (0%)]\tLoss: -0.006030\n",
      "Train Epoch: 0 [25600/50000 (51%)]\tLoss: -46.173103\n",
      "Epoch 1 took 2900.1922855377197 seconds to complete\n",
      "Test set: Accuracy: 4487/10000 (45%)\n",
      "\n",
      "Train Epoch: 1 [0/50000 (0%)]\tLoss: -131.752701\n",
      "Train Epoch: 1 [25600/50000 (51%)]\tLoss: -194.719177\n",
      "Epoch 2 took 2909.7070820331573 seconds to complete\n",
      "Test set: Accuracy: 5111/10000 (51%)\n",
      "\n",
      "Train Epoch: 2 [0/50000 (0%)]\tLoss: -296.520294\n",
      "Train Epoch: 2 [25600/50000 (51%)]\tLoss: -332.707611\n",
      "Epoch 3 took 2919.935966491699 seconds to complete\n",
      "Test set: Accuracy: 5362/10000 (54%)\n",
      "\n",
      "Train Epoch: 3 [0/50000 (0%)]\tLoss: -436.358795\n",
      "Train Epoch: 3 [25600/50000 (51%)]\tLoss: -388.855499\n",
      "Epoch 4 took 2931.5770175457 seconds to complete\n",
      "Test set: Accuracy: 5560/10000 (56%)\n",
      "\n",
      "Train Epoch: 4 [0/50000 (0%)]\tLoss: -454.591888\n",
      "Train Epoch: 4 [25600/50000 (51%)]\tLoss: -438.210266\n",
      "Epoch 5 took 2948.283804178238 seconds to complete\n",
      "Test set: Accuracy: 5640/10000 (56%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from large_margin import LargeMarginLoss\n",
    "\n",
    "\n",
    "lm = LargeMarginLoss(\n",
    "    gamma=10000,\n",
    "    alpha_factor=4,\n",
    "    top_k=num_training_classes,\n",
    "    dist_norm=np.inf\n",
    ")\n",
    "\n",
    "net = FeatureExtractor(net_type, num_training_classes)\n",
    "net.to(device)\n",
    "\n",
    "def train_lm(model, train_loader, optimizer, epoch, lm):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data = data.to(device)\n",
    "        one_hot = torch.zeros(len(target), 100).scatter_(1, target.unsqueeze(1), 1.).float()\n",
    "        one_hot = one_hot.cuda()\n",
    "        optimizer.zero_grad()\n",
    "        output, features = model(data)\n",
    "        for feature in features:\n",
    "            feature.retain_grad()\n",
    "\n",
    "        loss = lm(output, one_hot, features)\n",
    "        \n",
    "        wandb.log({\"loss\": loss})\n",
    "        # optional\n",
    "        wandb.watch(model)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.clear_features()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))\n",
    "\n",
    "import time\n",
    "\n",
    "optim = Adam(net.parameters()) #SGD(net.parameters(), lr=learning_rate, momentum=0)\n",
    "for i in range(0, epochs):\n",
    "    start_time = time.time()\n",
    "    train_lm(net, train_loader, optim, i, lm)\n",
    "    end_time = time.time()\n",
    "\n",
    "    print('Epoch {} took {} seconds to complete'.format(i+1, end_time-start_time))\n",
    "\n",
    "    test(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ce(model, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss_ce = F.cross_entropy(output, target)\n",
    "\n",
    "        # wandb.log({\"loss_ce\": loss_ce})\n",
    "        # # optional\n",
    "        # wandb.watch(model)\n",
    "\n",
    "        loss_ce.backward()\n",
    "        optimizer.step()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss_ce.item()))\n",
    "\n",
    "net = efficientnet_b1(pretrained=True).to(device)\n",
    "# net = nn.DataParallel(net).to(device)\n",
    "\n",
    "import time\n",
    "\n",
    "optim = Adam(net.parameters())\n",
    "for i in range(0, epochs):  \n",
    "    start_time = time.time()  \n",
    "    train_ce(net, train_loader, optim, i)\n",
    "    end_time = time.time()\n",
    "    print('Epoch {} took {} seconds to complete'.format(i+1, end_time-start_time))\n",
    "\n",
    "    test_ce(net, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3add45fc69f05cc29cf9d44865c28a2c1800260c2b05417b30b97f2c4a51861"
  },
  "kernelspec": {
   "display_name": "Python 3.6.8 ('project': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
